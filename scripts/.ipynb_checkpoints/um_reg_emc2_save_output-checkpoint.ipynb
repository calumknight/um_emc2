{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d0982c-52d7-496d-aac4-e6401652d791",
   "metadata": {},
   "source": [
    "# File saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fb72a8fa-f493-4bcf-bdec-ea489a32ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# um_emc2_main function\n",
    "##############################\n",
    "# The purpose of this function is to process and re-save UM regional model output so that it is ready to use with the Earth Model Column Collaboratory (EMC2) instrument simulator.\n",
    "# This function processes one day of data at daily or higher resolution. To process multiple days, we recommend constructing a wrapper for this function as follows:\n",
    "# for parent_folder, date, output_folder, latitudes, longitudes in list_of_inputs:\n",
    "#     um_emc2_main(parent_folder = parent_folder, date = date, output_folder = output_folder, latitudes = latitudes, longitudes = longitudes)\n",
    "# The output will a single .nc file saved to the output path at native model time and height resolution and with one horizontal grid cell of data.\n",
    "# The horizonal grid cell will either be a fixed point (len(latitudes) == len(longitudes) == 1) or a moving point (len(latitudes) ==  len(longitudes) > 1).\n",
    "# This function requires um_emc2_input_validator, um_emc2_dictionary_sorter, um_emc2_data_processor, and um_emc2_final_saver to execute suucessfully.\n",
    "##############################\n",
    "# Inputs:\n",
    "# a parent folder containing all data to read (the data files to read must be either .nc or .pp format);\n",
    "# a date for a single day as either a string ('YYYYMMDD') or datetime object;\n",
    "# an output path; and\n",
    "# separate decimal coordinates, either single latitude and longitude values (e.g. for a stationary location) or equi-dimensional latitude and longitude arrays for coordinate matching (e.g. to a moving ship)\n",
    "##############################\n",
    "\n",
    "# Import packages\n",
    "import os # for interacting with the operating system\n",
    "import re # for pattern matching\n",
    "import glob # for pattern matching in file lists\n",
    "import xarray as xr # for general dataset file handling\n",
    "import pandas as pd # for additional data handling\n",
    "import netCDF4 as nc # for NetCDF file handling\n",
    "import numpy as np # for scientific computing\n",
    "import emc2\n",
    "from emc2.core import Instrument\n",
    "from emc2.core.instrument import ureg\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the um_emc2_main function\n",
    "def um_emc2_main(date, parent_folder, output_folder, latitudes, longitudes):\n",
    "    \n",
    "    # FROM FUNCTIONS.PY IMPORT *, SCRIPT CONTAINING ALL FUNCTIONS CALLED BY MAIN\n",
    "    # IF COORDINATE ARRAY IS PROVIDED, A TIME ARRAY WILL BE REQUIRED ALONGISDE ALL COORDINATES\n",
    "\n",
    "    # Validate the input, including the date, parent and output folders, coordinates, and types of files to process, and return the lists of unsorted .nc and .pp files to process\n",
    "    date, date_str, files_list_nc, files_list_pp = um_emc2_input_validator(date = date, parent_folder = parent_folder, output_folder = output_folder, latitudes = latitudes, longitudes = longitudes)\n",
    "    print('um_emc2_input_validator executed successfully')\n",
    "    \n",
    "    # Pass file lists to the sorting function and return the sorted dictionary\n",
    "    filenames_dict = um_emc2_dictionary_sorter(files_list_nc = files_list_nc, files_list_pp = files_list_pp)\n",
    "    print('um_emc2_dictionary_sorter executed successfully')\n",
    "\n",
    "    # Process the data - extract only relevant variables, limit time and spatial domains, and save partial files in the output folder\n",
    "    um_emc2_data_processor(parent_folder = parent_folder, date = date, date_str = date_str, output_folder = output_folder, filenames_dict = filenames_dict, latitudes = latitudes, longitudes = longitudes)\n",
    "    print('um_emc2_data_processor executed successfully')\n",
    "\n",
    "    # Save the final data file for the selected day\n",
    "    um_emc2_final_saver(date_str, output_folder)\n",
    "    print('um_emc2_final_saver executed successfully')\n",
    "\n",
    "    print('um_emc2_main executed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a4df96e3-fcb7-485a-99b3-df132e8e4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the um_emc2_input_validator function\n",
    "def um_emc2_input_validator(date, parent_folder, output_folder, latitudes, longitudes):\n",
    "\n",
    "    # Date check\n",
    "    # If the date is a string, check whether the format is valid and convert it to datetime object\n",
    "    if isinstance(date, str):\n",
    "        try:\n",
    "            date = datetime.strptime(date, '%Y%m%d')\n",
    "        except ValueError:\n",
    "            raise ValueError('Invalid date format. Please provide a date in \"YYYYMMDD\" format')\n",
    "        date_str = date.strftime('%Y%m%d')\n",
    "    # If the date is a datetime, create a string version\n",
    "    elif isinstance(date, datetime):\n",
    "        date_str = date.strftime('%Y%m%d')\n",
    "    else:\n",
    "        raise ValueError('Input date must be a string or a datetime object')\n",
    "\n",
    "    # Parent folder check\n",
    "    if not os.path.isdir(parent_folder):\n",
    "        raise ValueError(f'parent_folder \"{parent_folder}\" is not a valid directory.')\n",
    "\n",
    "    # Output path check\n",
    "    if not os.path.isdir(output_folder):\n",
    "        raise ValueError(f'output_folder \"{output_folder}\" is not a valid directory.')\n",
    "\n",
    "    # Coordinates check\n",
    "    # Check if latitudes is a list\n",
    "    if isinstance(latitudes, list):\n",
    "        for lat in latitudes:\n",
    "            if not (-90 <= lat <= 90):\n",
    "                raise ValueError(f'Invalid latitude value: {lon}')\n",
    "    else:\n",
    "        # If latitudes is a single number\n",
    "        if not (-90 <= latitudes <= 90):\n",
    "            raise ValueError(f'Invalid latitude value: {longitudes}')\n",
    "    # Check if longitudes is a list\n",
    "    if isinstance(longitudes, list):\n",
    "        for lon in longitudes:\n",
    "            if not (0 <= lon <= 360):\n",
    "                raise ValueError(f'Invalid longitude value: {lon}')\n",
    "    else:\n",
    "        # If longitudes is a single number\n",
    "        if not (0 <= longitudes <= 360):\n",
    "            raise ValueError(f'Invalid longitude value: {longitudes}')\n",
    "\n",
    "    # File type check\n",
    "    # Create the list of all files in the parent folder\n",
    "    # Check what types of files need to be processed\n",
    "    files_list_nc = []\n",
    "    files_list_pp = []\n",
    "    for root, dirs, filenames in os.walk(parent_folder):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.nc'):\n",
    "                files_list_nc.append(os.path.join(root, filename))\n",
    "            elif not os.path.splitext(filename)[1]:\n",
    "                files_list_pp.append(os.path.join(root, filename))\n",
    "            else:\n",
    "                print(f'{filename} is an unrecognized filetype and will not be processed')\n",
    "    # Print numbers and types of files found\n",
    "    if files_list_nc:\n",
    "        print(f'{len(files_list_nc)} .nc files found', '\\n')\n",
    "    if files_list_pp:\n",
    "        # Iris is required to load .pp files\n",
    "        import iris\n",
    "        print(f'{len(files_list_pp)} .pp files found', '\\n')\n",
    "    if not files_list_nc and not files_list_pp:\n",
    "        raise ValueError('No files of either .nc or other type found')\n",
    "    if files_list_nc and files_list_pp:\n",
    "        print('Warning: Multiple file types detected; code will proceeed, but we recommend ensuring uniformity of file types for simplicity of error diagnosis')\n",
    "\n",
    "    print(date)\n",
    "    print(date_str)\n",
    "    print()\n",
    "    return(date, date_str, files_list_nc, files_list_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b08275c-0541-4df1-8f10-dc9b83ad4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the um_emc2_dictionary_sorter function\n",
    "def um_emc2_dictionary_sorter(files_list_nc, files_list_pp):\n",
    "\n",
    "    # Create the dictionary of files sorted by numerical code (presumably time) in the base filename\n",
    "    # Create a unified files list\n",
    "    files_list_all = files_list_nc + files_list_pp\n",
    "    filenames_list = [os.path.basename(filename) for filename in files_list_all]\n",
    "    # Initialize the dictionary\n",
    "    filenames_dict = {}\n",
    "    for filename in filenames_list:\n",
    "        # If the file type .nc, strip the extension first\n",
    "        if filename.endswith('.nc'):\n",
    "            filename_base = os.path.splitext(filename)[0]\n",
    "        # If the file type is absent, the file is presumed .pp type\n",
    "        else:\n",
    "            filename_base = filename\n",
    "        # If the numeric block is already a key in the dictionary, append the filename including the extension\n",
    "        # Otherwise, create a new list with the filename as the value for that key\n",
    "        numeric_blocks = re.findall(r'\\d+', filename_base)\n",
    "        for numeric_block in numeric_blocks:\n",
    "            if numeric_block in filenames_dict:\n",
    "                filenames_dict[numeric_block].append(filename)\n",
    "            else:\n",
    "                filenames_dict[numeric_block] = [filename]\n",
    "    # Sort filenames within each numeric block\n",
    "    for numeric_block, filenames in filenames_dict.items():\n",
    "        filenames_dict[numeric_block] = sorted(filenames)\n",
    "    # Sort blocks/keys by number\n",
    "    filenames_dict = dict(sorted(filenames_dict.items(), key=lambda item: int(item[0])))\n",
    "    # Print the keys and files\n",
    "    for key, items in filenames_dict.items():\n",
    "        print(key)\n",
    "        for item in items:\n",
    "            print(item)\n",
    "    print()\n",
    "    return(filenames_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "90777ac7-51c3-4432-afc6-e0b728a17c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the um_emc2_data_processor function\n",
    "def um_emc2_data_processor(parent_folder, date, date_str, output_folder, filenames_dict, latitudes, longitudes):\n",
    "\n",
    "    # Create the list of variables to look for\n",
    "    required_variables = ['mass_fraction_of_cloud_liquid_water_in_air','mass_fraction_of_cloud_ice_in_air','mass_fraction_of_rain_in_air','mass_fraction_of_cloud_ice_crystals_in_air','mass_fraction_of_graupel_in_air',\n",
    "    'number_of_cloud_droplets_per_kg_of_air','number_of_ice_particles_per_kg_of_air','number_of_rain_drops_per_kg_of_air','number_of_snow_aggregates_per_kg_of_air','number_of_graupel_particles_per_kg_of_air',\n",
    "    'cloud_area_fraction_in_atmosphere_layer','air_pressure','air_temperature','specific_humidity']\n",
    "    required_coordinates = ['level_height','time','model_level_number','grid_latitude','grid_longitude']\n",
    "    variables_to_keep = required_variables + required_coordinates\n",
    "    # Loop across all file groups defined by dictionary keys\n",
    "    for key in list(filenames_dict.keys()):\n",
    "        # Create the list of current_files\n",
    "        current_items = filenames_dict[key]\n",
    "        # Identify the current key\n",
    "        print(f'Now searching {key} files...')\n",
    "        # Reset the key_dataset\n",
    "        key_dataset = xr.Dataset()\n",
    "        # Reset the breaker\n",
    "        break_current = False\n",
    "        # Reset the list of variables found\n",
    "        variables_found = []\n",
    "        # Loop across all current files\n",
    "        for file in current_items:\n",
    "            # Check the file type and handle accordingly\n",
    "            # If the filetype is .nc...\n",
    "            if file.endswith('.nc'):\n",
    "                # Load the current dataset\n",
    "                dataset = xr.open_dataset(f'{parent_folder}/{file}')\n",
    "                print(f'Now searching {file}...')\n",
    "                # Check whether the data contains a time coordinate\n",
    "                if 'time' not in dataset:\n",
    "                    print('No time coordinate found - skipping current file')\n",
    "                    continue\n",
    "                # Remove the time component from the date\n",
    "                date_only = np.datetime64(date, 'D')\n",
    "                \n",
    "                print()\n",
    "                print(date)\n",
    "                print(date_str)\n",
    "                print(date_only)\n",
    "                \n",
    "                print()\n",
    "                # Select time values within the current date\n",
    "                time_within_date = dataset.time.where((dataset.time.dt.floor('D') == date_only), drop=True)\n",
    "                # Check whether dataset uses time as an index\n",
    "                try:\n",
    "                    dataset = dataset.sel(time=time_within_date)\n",
    "                except KeyError:\n",
    "                    print('No index found for coordinate time - skipping current file')\n",
    "                    continue\n",
    "                # Check whether any data remains\n",
    "                if len(dataset.time) == 0:\n",
    "                    print(f'{file} in {key} block has no data inside current date - skipping current key...')\n",
    "                    break_current = True\n",
    "                    break\n",
    "                # Check whether the dataset contains any of the required variables\n",
    "                if any(var in dataset.variables for var in required_variables):\n",
    "                    # Drop variables not in required_variables or required_coordinates\n",
    "                    dataset = dataset.drop_vars([var for var in dataset.variables if var not in variables_to_keep])\n",
    "                    # Loop across the variables in the dataset\n",
    "                    for var in dataset:\n",
    "                        # Is the variable required?\n",
    "                        if var in required_variables:\n",
    "                            # Has the variable already been found in the current key?\n",
    "                            if var in variables_found:\n",
    "                                # Drop the variable from the dataset\n",
    "                                dataset = dataset.drop_vars(var)\n",
    "                            else:\n",
    "                                print(f'{var} found in {file}')\n",
    "                                # Add the variable to the list of found variables\n",
    "                                variables_found += [var]\n",
    "                    # Check whether the dataset still contains any of the required variables\n",
    "                    if any(var in dataset.variables for var in required_variables):\n",
    "                        # Merge the dataset with the key_dataset\n",
    "        \n",
    "                        # TEMPORARY SUBSETTING TO ENSURE TESTABILITY\n",
    "                        if 'lat' in dataset.variables and 'lon' in dataset.variables:\n",
    "                            subset = dataset.sel(lat=0, lon=0, method='nearest')\n",
    "                            key_dataset = key_dataset.merge(subset)\n",
    "                            print(f'File {file} merged')\n",
    "                        elif 'grid_latitude' in dataset.variables and 'grid_longitude' in dataset.variables:\n",
    "                            subset = dataset.sel(grid_latitude=0, grid_longitude=0, method='nearest')\n",
    "                            key_dataset = key_dataset.merge(subset)\n",
    "                            print(f'File {file} merged')\n",
    "                        else:\n",
    "                            print(f'{file} does not use the required coordinates')\n",
    "                            continue\n",
    "    \n",
    "                    else:\n",
    "                        # No variables found?\n",
    "                        print(f'{file} contains no required variables')\n",
    "                else:\n",
    "                    # No variables found?\n",
    "                    print(f'{file} contains no required variables')\n",
    "            # If the filetype is not .nc...\n",
    "            else:\n",
    "                # Load the current dataset using iris\n",
    "                iris_cubes = iris.load(f'{parent_folder}/{file}')\n",
    "                # Initialize an empty dataset\n",
    "                dataset = xr.Dataset()\n",
    "                print(f'Now searching {file}...')\n",
    "                # Iterate over the variables in iris_cubes to create the dataset\n",
    "                for i, f in enumerate(iris_cubes):\n",
    "                    # Create the temporary dataset\n",
    "                    temp_dataset = xr.DataArray.from_iris(iris_cubes[i])                \n",
    "                    # Check whether the temp_dataset current variable is required\n",
    "                    if temp_dataset.name in required_variables:\n",
    "                        # Check whether the variable has already been found\n",
    "                        if temp_dataset.name not in variables_found:\n",
    "                            try:\n",
    "                                dataset = dataset.merge(temp_dataset)\n",
    "                            except xr.MergeError as e:\n",
    "                                if 'conflicting values for variable \\'level_height\\'' in str(e):\n",
    "                                    print(f'Warning: Conflicing values detected for variable \"level_height\" in {temp_dataset.name}; compat=\"override\" used')\n",
    "                                    dataset = dataset.merge(temp_dataset, compat='override')\n",
    "                                elif 'conflicting values for variable \\'forecast_period\\'' in str(e):\n",
    "                                    print(f'Warning: Conflicing values detected for variable \"forecast_period\" in {temp_dataset.name}; compat=\"override\" used')\n",
    "                                    dataset = dataset.merge(temp_dataset, compat='override')\n",
    "                                elif 'conflicting values for variable \\'forecast_reference_time\\'' in str(e):\n",
    "                                    print(f'Warning: Conflicing values detected for variable \"forecast_reference_time\" in {temp_dataset.name}; compat=\"override\" used')\n",
    "                                    dataset = dataset.merge(temp_dataset, compat='override')\n",
    "                                elif 'conflicting values for variable \\'height\\'' in str(e):\n",
    "                                    print(f'Warning: Conflicing values detected for variable \"height\" in {temp_dataset.name}; compat=\"override\" used')\n",
    "                                    dataset = dataset.merge(temp_dataset, compat='override')\n",
    "                                elif 'conflicting values for variable \\'level_height\\'' in str(e):\n",
    "                                    print(f'Warning: Conflicing values detected for variable \"level_height\" in {temp_dataset.name}; compat=\"override\" used')\n",
    "                                    dataset = dataset.merge(temp_dataset, compat='override')\n",
    "                                else:\n",
    "                                    raise e\n",
    "                            except AttributeError as e:\n",
    "                                print(f\"Skipping variable {temp_dataset.name}: {e}\")\n",
    "                                continue  # Skip this iteration and continue with the next one\n",
    "                            except ValueError as e:\n",
    "                                if \"Unpacking PP fields with LBPACK of 1 requires mo_pack to be installed\" in str(e):\n",
    "                                    print(\"Warning: mo_pack is not installed; skipping the current loop iteration\")\n",
    "                                    continue  # Skip the current iteration and move to the next one\n",
    "                                else:\n",
    "                                    raise # Re-raise the error if it's not the expected one\n",
    "                        print(f'{temp_dataset.name} found in {file}')\n",
    "                        # Add the variable to the list of found variables\n",
    "                        variables_found += [temp_dataset.name]\n",
    "                        # Identify the variable found\n",
    "                # Check whether the data contains a time coordinate\n",
    "                if 'time' not in dataset.variables:\n",
    "                    print('No time coordinate found - skipping current file')\n",
    "                    continue\n",
    "                # Remove the time component from the date\n",
    "                date_only = np.datetime64(date, 'D')\n",
    "                # Select time values within the current day\n",
    "                time_within_date = dataset.time.where((dataset.time.dt.floor('D') == date_only), drop=True)\n",
    "                # Check whether dataset uses time as an index\n",
    "                try:\n",
    "                    dataset = dataset.sel(time=time_within_date)\n",
    "                except KeyError:\n",
    "                    print('No index found for coordinate time - skipping current file')\n",
    "                    continue\n",
    "                # Check whether any data remains\n",
    "                if len(dataset.time) == 0:\n",
    "                    print(f'{file} in {key} block has no data inside current date - skipping current key...')\n",
    "                    break_current = True\n",
    "                    break\n",
    "                # Drop variables not in required_variables or required_coordinates\n",
    "                dataset = dataset.drop_vars([var for var in dataset.variables if var not in variables_to_keep])\n",
    "                # Check whether the dataset still contains any of the required variables\n",
    "                if any(var in dataset.variables for var in required_variables):\n",
    "                    # Merge the dataset with the key_dataset\n",
    "        \n",
    "                    # TEMPORARY SUBSETTING TO ENSURE TESTABILITY\n",
    "                    if 'lat' in dataset.variables and 'lon' in dataset.variables:\n",
    "                        subset = dataset.sel(lat=0, lon=0, method='nearest')\n",
    "                        key_dataset = key_dataset.merge(subset)\n",
    "                        print(f'File {file} merged')\n",
    "                    elif 'grid_latitude' in dataset.variables and 'grid_longitude' in dataset.variables:\n",
    "                        subset = dataset.sel(grid_latitude=0, grid_longitude=0, method='nearest')\n",
    "                        key_dataset = key_dataset.merge(subset)\n",
    "                        print(f'File {file} merged')\n",
    "                    else:\n",
    "                        print(f'{file} does not use the required coordinates')\n",
    "                        continue\n",
    "                else:\n",
    "                    # No variables found?\n",
    "                    print(f'{file} contains no required variables')\n",
    "        \n",
    "        # If there are no data in the current date, skip the current key\n",
    "        if not break_current:\n",
    "            # Set the save path and save the data\n",
    "            print(f'Saving the {key} data...')\n",
    "            save_path = os.path.join(output_folder, f'um_emc2_test_{date_str}_{key}.nc')\n",
    "            key_dataset.to_netcdf(save_path)\n",
    "            print(f'File saved: {save_path}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "28116324-2719-4058-8547-e697f821e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the um_emc2_final_saver function\n",
    "def um_emc2_final_saver(date_str, output_folder):\n",
    "\n",
    "    # Define the pattern of partial files to search for\n",
    "    pattern = os.path.join(output_folder, f'*{date_str}*')\n",
    "    # List all partial files for the current date\n",
    "    saved_partial_files = glob.glob(pattern)\n",
    "    # Initialize the final dataset\n",
    "    final_dataset = xr.Dataset()\n",
    "    # Load each partial file and merge into the final dataset\n",
    "    for file in saved_partial_files:\n",
    "        temp_dataset = xr.open_dataset(file)\n",
    "        final_dataset = final_dataset.merge(temp_dataset)\n",
    "    \n",
    "    # Create a variable filled with zeros and add to the dataset\n",
    "    zeros_data = xr.DataArray(\n",
    "        data = np.zeros((len(final_dataset['time']), len(final_dataset['level_height']))),\n",
    "        dims = ('time', 'model_level_number'),\n",
    "        coords = {'time': final_dataset['time'], 'level_height': final_dataset['level_height']}\n",
    "    )\n",
    "    final_dataset['zeros_var'] = zeros_data\n",
    "    \n",
    "    # Add a 2-dimensional z_values variable to the dataset\n",
    "    level_height = final_dataset['level_height'].values\n",
    "    height_data = level_height[:, np.newaxis] * np.ones(len(final_dataset['time']))\n",
    "    final_dataset['height_var'] = (('level_height', 'time'), height_data)\n",
    "    final_dataset['height_var'].attrs['units'] = 'meter'\n",
    "    final_dataset['height_var'] = final_dataset['height_var'].transpose('time', 'level_height')\n",
    "    \n",
    "    # Add units to all fields\n",
    "    # Mass mixing ratio\n",
    "    final_dataset['mass_fraction_of_cloud_liquid_water_in_air'] = final_dataset['mass_fraction_of_cloud_liquid_water_in_air'].assign_attrs(units=ureg.kg/ureg.kg)\n",
    "    final_dataset['mass_fraction_of_cloud_liquid_water_in_air'].attrs['units'] = str(final_dataset['mass_fraction_of_cloud_liquid_water_in_air'].attrs['units'])\n",
    "    final_dataset['mass_fraction_of_cloud_ice_crystals_in_air'] = final_dataset['mass_fraction_of_cloud_ice_crystals_in_air'].assign_attrs(units=ureg.kg/ureg.kg)\n",
    "    final_dataset['mass_fraction_of_cloud_ice_crystals_in_air'].attrs['units'] = str(final_dataset['mass_fraction_of_cloud_ice_crystals_in_air'].attrs['units'])\n",
    "    final_dataset['mass_fraction_of_rain_in_air'] = final_dataset['mass_fraction_of_rain_in_air'].assign_attrs(units=ureg.kg/ureg.kg)\n",
    "    final_dataset['mass_fraction_of_rain_in_air'].attrs['units'] = str(final_dataset['mass_fraction_of_rain_in_air'].attrs['units'])\n",
    "    final_dataset['mass_fraction_of_cloud_ice_in_air'] = final_dataset['mass_fraction_of_cloud_ice_in_air'].assign_attrs(units=ureg.kg/ureg.kg)\n",
    "    final_dataset['mass_fraction_of_cloud_ice_in_air'].attrs['units'] = str(final_dataset['mass_fraction_of_cloud_ice_in_air'].attrs['units'])\n",
    "    final_dataset['mass_fraction_of_graupel_in_air'] = final_dataset['mass_fraction_of_graupel_in_air'].assign_attrs(units=ureg.kg/ureg.kg)\n",
    "    final_dataset['mass_fraction_of_graupel_in_air'].attrs['units'] = str(final_dataset['mass_fraction_of_graupel_in_air'].attrs['units'])\n",
    "    # Number concentration\n",
    "    # Add number concentration per kilogram of air to final_dataset\n",
    "    final_dataset['number_of_cloud_droplets_per_kg_of_air'] = final_dataset['number_of_cloud_droplets_per_kg_of_air']\n",
    "    final_dataset['number_of_ice_particles_per_kg_of_air'] = final_dataset['number_of_ice_particles_per_kg_of_air']\n",
    "    final_dataset['number_of_rain_drops_per_kg_of_air'] = final_dataset['number_of_rain_drops_per_kg_of_air']\n",
    "    final_dataset['number_of_snow_aggregates_per_kg_of_air'] = final_dataset['number_of_snow_aggregates_per_kg_of_air']\n",
    "    final_dataset['number_of_graupel_particles_per_kg_of_air'] = final_dataset['number_of_graupel_particles_per_kg_of_air']\n",
    "    # Add units\n",
    "    final_dataset['number_of_cloud_droplets_per_kg_of_air'] = final_dataset['number_of_cloud_droplets_per_kg_of_air'].assign_attrs(units=ureg.kg**(-1))\n",
    "    final_dataset['number_of_cloud_droplets_per_kg_of_air'].attrs['units'] = str(final_dataset['number_of_cloud_droplets_per_kg_of_air'].attrs['units'])\n",
    "    final_dataset['number_of_ice_particles_per_kg_of_air'] = final_dataset['number_of_ice_particles_per_kg_of_air'].assign_attrs(units=ureg.kg**(-1))\n",
    "    final_dataset['number_of_ice_particles_per_kg_of_air'].attrs['units'] = str(final_dataset['number_of_ice_particles_per_kg_of_air'].attrs['units'])\n",
    "    final_dataset['number_of_rain_drops_per_kg_of_air'] = final_dataset['number_of_rain_drops_per_kg_of_air'].assign_attrs(units=ureg.kg**(-1))\n",
    "    final_dataset['number_of_rain_drops_per_kg_of_air'].attrs['units'] = str(final_dataset['number_of_rain_drops_per_kg_of_air'].attrs['units'])\n",
    "    final_dataset['number_of_snow_aggregates_per_kg_of_air'] = final_dataset['number_of_snow_aggregates_per_kg_of_air'].assign_attrs(units=ureg.kg**(-1))\n",
    "    final_dataset['number_of_snow_aggregates_per_kg_of_air'].attrs['units'] = str(final_dataset['number_of_snow_aggregates_per_kg_of_air'].attrs['units'])\n",
    "    final_dataset['number_of_graupel_particles_per_kg_of_air'] = final_dataset['number_of_graupel_particles_per_kg_of_air'].assign_attrs(units=ureg.kg**(-1))\n",
    "    final_dataset['number_of_graupel_particles_per_kg_of_air'].attrs['units'] = str(final_dataset['number_of_graupel_particles_per_kg_of_air'].attrs['units'])\n",
    "    # Stratiform fraction\n",
    "    final_dataset['cloud_area_fraction_in_atmosphere_layer'] = final_dataset['cloud_area_fraction_in_atmosphere_layer'].assign_attrs(units=ureg.meter/ureg.meter)\n",
    "    final_dataset['cloud_area_fraction_in_atmosphere_layer'].attrs['units'] = str(final_dataset['cloud_area_fraction_in_atmosphere_layer'].attrs['units'])\n",
    "    # Pressure\n",
    "    final_dataset['air_pressure'] = final_dataset['air_pressure'].assign_attrs(units=ureg.pascal)\n",
    "    final_dataset['air_pressure'].attrs['units'] = str(final_dataset['air_pressure'].attrs['units'])\n",
    "    # Temperature\n",
    "    final_dataset['air_temperature'] = final_dataset['air_temperature'].assign_attrs(units=ureg.kelvin)\n",
    "    final_dataset['air_temperature'].attrs['units'] = str(final_dataset['air_temperature'].attrs['units'])\n",
    "    # Specific humidity\n",
    "    final_dataset['specific_humidity'] = final_dataset['specific_humidity']\n",
    "    final_dataset['specific_humidity'] = final_dataset['specific_humidity'].assign_attrs(units=ureg.kg/ureg.kg)\n",
    "    final_dataset['specific_humidity'].attrs['units'] = str(final_dataset['specific_humidity'].attrs['units'])\n",
    "    \n",
    "    # Change primary height dimension to use level_height values\n",
    "    level_heights = final_dataset['level_height']\n",
    "    final_dataset['model_level_number'] = level_heights\n",
    "    # Remove/rename coordinates\n",
    "    # final_dataset = final_dataset.drop_vars(['level_height', 'sigma', 'forecast_reference_time'])\n",
    "    final_dataset = final_dataset.drop_vars(['level_height'])\n",
    "    final_dataset = final_dataset.rename({'model_level_number': 'level_height'})\n",
    "    \n",
    "    # Add general attributes to the dataset\n",
    "    # final_dataset.attrs['description'] = 'UM regional model data subsetted to the MARCUS RSV location'\n",
    "    # final_dataset.attrs['authors'] = 'Calum L. Knight & Sonya L. Fiddes'\n",
    "    final_dataset.attrs['creation_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Set the final filename\n",
    "    final_filename = f'um_emc2_test_{date_str}.nc'\n",
    "    # Save the final dataset\n",
    "    final_dataset.to_netcdf(os.path.join(output_folder, final_filename))\n",
    "\n",
    "    # Delete partial files\n",
    "    for file in saved_partial_files:\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f810e53-d5f9-4923-a8ae-f878e202c176",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4957d634-828f-4084-a0da-ffcc213944bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date\n",
    "# date = datetime(2018, 2, 4)\n",
    "date = '20180204'\n",
    "\n",
    "# Parent folder\n",
    "parent_folder = '/g/data/jk72/ck4840/projects/um_emc2/data/um_reg_input_files'\n",
    "# parent_folder = '/g/data/jk72/slf563/cylc-run/u-db930/share/cycle/20180203T1200Z/Mawson/resn_1/RAL3p2/um/processed'\n",
    "\n",
    "# Output folder\n",
    "output_folder = '/g/data/jk72/ck4840/projects/um_emc2/data/output'\n",
    "\n",
    "# Coordinates\n",
    "coordinates = xr.open_dataset('/g/data/jk72/ck4840/projects/emc2/data/marcus_coordinates/marcus_coordinates_20180201_20180207_1min.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2d777e70-a779-4d1c-b58f-aad41c3e5c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime_1min = coordinates['datetime_1min']\n",
    "# coordinates['index'] = datetime_1min\n",
    "# coordinates = coordinates.drop_vars('datetime_1min')\n",
    "# coordinates = coordinates.rename({'index': 'datetime_1min'})coordinates.to_netcdf('/g/data/jk72/ck4840/projects/emc2/data/marcus_coordinates_20180201_20180207_mod_1min.nc')\n",
    "# coordinates.to_netcdf('/g/data/jk72/ck4840/projects/emc2/data/marcus_coordinates_20180201_20180207_mod_1min.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a4ff5b50-c34e-49f7-bd71-1f2d86dddec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:        (datetime_1min: 1008)\n",
       "Coordinates:\n",
       "  * datetime_1min  (datetime_1min) object &#x27;2018-02-01 00:01:00&#x27; ... &#x27;2018-02-...\n",
       "Data variables:\n",
       "    longitude      (datetime_1min) float64 ...\n",
       "    latitude       (datetime_1min) float64 ...\n",
       "    voyage_id      (datetime_1min) int64 ...\n",
       "    datetime_day   (datetime_1min) datetime64[ns] ...</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-319bbc9e-ce41-4736-ab7a-ac1761bdc124' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-319bbc9e-ce41-4736-ab7a-ac1761bdc124' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>datetime_1min</span>: 1008</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-7ff1a30c-b522-4ea6-98ca-5c861c0135f4' class='xr-section-summary-in' type='checkbox'  checked><label for='section-7ff1a30c-b522-4ea6-98ca-5c861c0135f4' class='xr-section-summary' >Coordinates: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>datetime_1min</span></div><div class='xr-var-dims'>(datetime_1min)</div><div class='xr-var-dtype'>object</div><div class='xr-var-preview xr-preview'>&#x27;2018-02-01 00:01:00&#x27; ... &#x27;2018-...</div><input id='attrs-36ffde9b-7b3f-4071-9dad-a63f1df0d7d0' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-36ffde9b-7b3f-4071-9dad-a63f1df0d7d0' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-2d21b2b5-5ef6-474a-b1c8-0d2a13ba9d41' class='xr-var-data-in' type='checkbox'><label for='data-2d21b2b5-5ef6-474a-b1c8-0d2a13ba9d41' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;2018-02-01 00:01:00&#x27;, &#x27;2018-02-01 00:11:00&#x27;, &#x27;2018-02-01 00:21:00&#x27;,\n",
       "       ..., &#x27;2018-02-07 23:31:00&#x27;, &#x27;2018-02-07 23:41:00&#x27;,\n",
       "       &#x27;2018-02-07 23:51:00&#x27;], dtype=object)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-987f33b8-55e5-474b-b7da-e0938ccb000e' class='xr-section-summary-in' type='checkbox'  checked><label for='section-987f33b8-55e5-474b-b7da-e0938ccb000e' class='xr-section-summary' >Data variables: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>longitude</span></div><div class='xr-var-dims'>(datetime_1min)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-a5d06108-206f-4839-a7a3-633bb77bdd26' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-a5d06108-206f-4839-a7a3-633bb77bdd26' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f45f4a78-1a47-4179-902a-030b591576eb' class='xr-var-data-in' type='checkbox'><label for='data-f45f4a78-1a47-4179-902a-030b591576eb' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1008 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>latitude</span></div><div class='xr-var-dims'>(datetime_1min)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-999054be-36e3-4d3d-8fb4-fc9ad4af07e8' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-999054be-36e3-4d3d-8fb4-fc9ad4af07e8' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-4e3f7911-ba7e-42f3-96d7-5a4565d82ae6' class='xr-var-data-in' type='checkbox'><label for='data-4e3f7911-ba7e-42f3-96d7-5a4565d82ae6' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1008 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>voyage_id</span></div><div class='xr-var-dims'>(datetime_1min)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-f839dcb9-24aa-48de-97f4-c85418dbbf8b' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-f839dcb9-24aa-48de-97f4-c85418dbbf8b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-57b625b1-95ec-4a13-b106-2aba6694681d' class='xr-var-data-in' type='checkbox'><label for='data-57b625b1-95ec-4a13-b106-2aba6694681d' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1008 values with dtype=int64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>datetime_day</span></div><div class='xr-var-dims'>(datetime_1min)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-a9e6d447-d45c-45f0-b49c-ea60e8b69799' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-a9e6d447-d45c-45f0-b49c-ea60e8b69799' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6c2f6420-da4a-440c-917c-357e09b4a072' class='xr-var-data-in' type='checkbox'><label for='data-6c2f6420-da4a-440c-917c-357e09b4a072' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1008 values with dtype=datetime64[ns]]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-8c9172db-3874-4919-bdeb-c50b3fa9201b' class='xr-section-summary-in' type='checkbox'  ><label for='section-8c9172db-3874-4919-bdeb-c50b3fa9201b' class='xr-section-summary' >Indexes: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>datetime_1min</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-d1b6cd33-00ad-44cc-82b8-ffcfa436f2d5' class='xr-index-data-in' type='checkbox'/><label for='index-d1b6cd33-00ad-44cc-82b8-ffcfa436f2d5' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([&#x27;2018-02-01 00:01:00&#x27;, &#x27;2018-02-01 00:11:00&#x27;, &#x27;2018-02-01 00:21:00&#x27;,\n",
       "       &#x27;2018-02-01 00:31:00&#x27;, &#x27;2018-02-01 00:41:00&#x27;, &#x27;2018-02-01 00:51:00&#x27;,\n",
       "       &#x27;2018-02-01 01:01:00&#x27;, &#x27;2018-02-01 01:11:00&#x27;, &#x27;2018-02-01 01:21:00&#x27;,\n",
       "       &#x27;2018-02-01 01:31:00&#x27;,\n",
       "       ...\n",
       "       &#x27;2018-02-07 22:21:00&#x27;, &#x27;2018-02-07 22:31:00&#x27;, &#x27;2018-02-07 22:41:00&#x27;,\n",
       "       &#x27;2018-02-07 22:51:00&#x27;, &#x27;2018-02-07 23:01:00&#x27;, &#x27;2018-02-07 23:11:00&#x27;,\n",
       "       &#x27;2018-02-07 23:21:00&#x27;, &#x27;2018-02-07 23:31:00&#x27;, &#x27;2018-02-07 23:41:00&#x27;,\n",
       "       &#x27;2018-02-07 23:51:00&#x27;],\n",
       "      dtype=&#x27;object&#x27;, name=&#x27;datetime_1min&#x27;, length=1008))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-d6a29fad-41a8-4ad3-a59e-cdb90c7e4b1c' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-d6a29fad-41a8-4ad3-a59e-cdb90c7e4b1c' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (datetime_1min: 1008)\n",
       "Coordinates:\n",
       "  * datetime_1min  (datetime_1min) object '2018-02-01 00:01:00' ... '2018-02-...\n",
       "Data variables:\n",
       "    longitude      (datetime_1min) float64 ...\n",
       "    latitude       (datetime_1min) float64 ...\n",
       "    voyage_id      (datetime_1min) int64 ...\n",
       "    datetime_day   (datetime_1min) datetime64[ns] ..."
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "12a756e9-17ea-4b1d-bd5b-8a4bc51f0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date check\n",
    "# If the date is a string, check whether the format is valid and convert it to datetime object\n",
    "if isinstance(date, str):\n",
    "    try:\n",
    "        date = datetime.strptime(date, '%Y%m%d')\n",
    "    except ValueError:\n",
    "        raise ValueError('Invalid date format. Please provide a date in \"YYYYMMDD\" format')\n",
    "    date_str = date.strftime('%Y%m%d')\n",
    "# If the date is a datetime, create a string version\n",
    "elif isinstance(date, datetime):\n",
    "    date_str = date.strftime('%Y%m%d')\n",
    "else:\n",
    "    raise ValueError('Input date must be a string or a datetime object')\n",
    "\n",
    "# Parent folder check\n",
    "if not os.path.isdir(parent_folder):\n",
    "    raise ValueError(f'parent_folder \"{parent_folder}\" is not a valid directory.')\n",
    "\n",
    "# Output path check\n",
    "if not os.path.isdir(output_folder):\n",
    "    raise ValueError(f'output_folder \"{output_folder}\" is not a valid directory.')\n",
    "\n",
    "# Coordinates check\n",
    "# Check if latitudes is a list\n",
    "if isinstance(latitudes, list):\n",
    "    for lat in latitudes:\n",
    "        if not (-90 <= lat <= 90):\n",
    "            raise ValueError(f'Invalid latitude value: {lon}')\n",
    "else:\n",
    "    # If latitudes is a single number\n",
    "    if not (-90 <= latitudes <= 90):\n",
    "        raise ValueError(f'Invalid latitude value: {longitudes}')\n",
    "# Check if longitudes is a list\n",
    "if isinstance(longitudes, list):\n",
    "    for lon in longitudes:\n",
    "        if not (0 <= lon <= 360):\n",
    "            raise ValueError(f'Invalid longitude value: {lon}')\n",
    "else:\n",
    "    # If longitudes is a single number\n",
    "    if not (0 <= longitudes <= 360):\n",
    "        raise ValueError(f'Invalid longitude value: {longitudes}')\n",
    "\n",
    "# File type check\n",
    "# Create the list of all files in the parent folder\n",
    "# Check what types of files need to be processed\n",
    "files_list_nc = []\n",
    "files_list_pp = []\n",
    "for root, dirs, filenames in os.walk(parent_folder):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.nc'):\n",
    "            files_list_nc.append(os.path.join(root, filename))\n",
    "        elif not os.path.splitext(filename)[1]:\n",
    "            files_list_pp.append(os.path.join(root, filename))\n",
    "        else:\n",
    "            print(f'{filename} is an unrecognized filetype and will not be processed')\n",
    "# Print numbers and types of files found\n",
    "if files_list_nc:\n",
    "    print(f'{len(files_list_nc)} .nc files found', '\\n')\n",
    "if files_list_pp:\n",
    "    # Iris is required to load .pp files\n",
    "    import iris\n",
    "    print(f'{len(files_list_pp)} .pp files found', '\\n')\n",
    "if not files_list_nc and not files_list_pp:\n",
    "    raise ValueError('No files of either .nc or other type found')\n",
    "if files_list_nc and files_list_pp:\n",
    "    print('Warning: Multiple file types detected; code will proceeed, but we recommend ensuring uniformity of file types for simplicity of error diagnosis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "14e43f2a-ef46-4fd9-a59e-5606b563e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary of files sorted by numerical code (presumably time) in the base filename\n",
    "# Create a unified files list\n",
    "files_list_all = files_list_nc + files_list_pp\n",
    "filenames_list = [os.path.basename(filename) for filename in files_list_all]\n",
    "# Initialize the dictionary\n",
    "filenames_dict = {}\n",
    "for filename in filenames_list:\n",
    "    # If the file type .nc, strip the extension first\n",
    "    if filename.endswith('.nc'):\n",
    "        filename_base = os.path.splitext(filename)[0]\n",
    "    # If the file type is absent, the file is presumed .pp type\n",
    "    else:\n",
    "        filename_base = filename\n",
    "    # If the numeric block is already a key in the dictionary, append the filename including the extension\n",
    "    # Otherwise, create a new list with the filename as the value for that key\n",
    "    numeric_blocks = re.findall(r'\\d+', filename_base)\n",
    "    for numeric_block in numeric_blocks:\n",
    "        if numeric_block in filenames_dict:\n",
    "            filenames_dict[numeric_block].append(filename)\n",
    "        else:\n",
    "            filenames_dict[numeric_block] = [filename]\n",
    "# Sort filenames within each numeric block\n",
    "for numeric_block, filenames in filenames_dict.items():\n",
    "    filenames_dict[numeric_block] = sorted(filenames)\n",
    "# Sort blocks/keys by number\n",
    "filenames_dict = dict(sorted(filenames_dict.items(), key=lambda item: int(item[0])))\n",
    "# Print the keys and files\n",
    "for key, items in filenames_dict.items():\n",
    "    print(key)\n",
    "    for item in items:\n",
    "        print(item)\n",
    "print()\n",
    "return(filenames_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f1cc4-1370-40f2-be65-61debd9ed03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of variables to look for\n",
    "required_variables = ['mass_fraction_of_cloud_liquid_water_in_air','mass_fraction_of_cloud_ice_in_air','mass_fraction_of_rain_in_air','mass_fraction_of_cloud_ice_crystals_in_air','mass_fraction_of_graupel_in_air',\n",
    "'number_of_cloud_droplets_per_kg_of_air','number_of_ice_particles_per_kg_of_air','number_of_rain_drops_per_kg_of_air','number_of_snow_aggregates_per_kg_of_air','number_of_graupel_particles_per_kg_of_air',\n",
    "'cloud_area_fraction_in_atmosphere_layer','air_pressure','air_temperature','specific_humidity']\n",
    "required_coordinates = ['level_height','time','model_level_number','grid_latitude','grid_longitude']\n",
    "variables_to_keep = required_variables + required_coordinates\n",
    "# Loop across all file groups defined by dictionary keys\n",
    "for key in list(filenames_dict.keys()):\n",
    "    # Create the list of current_files\n",
    "    current_items = filenames_dict[key]\n",
    "    # Identify the current key\n",
    "    print(f'Now searching {key} files...')\n",
    "    # Reset the key_dataset\n",
    "    key_dataset = xr.Dataset()\n",
    "    # Reset the breaker\n",
    "    break_current = False\n",
    "    # Reset the list of variables found\n",
    "    variables_found = []\n",
    "    # Loop across all current files\n",
    "    for file in current_items:\n",
    "        # Check the file type and handle accordingly\n",
    "        # If the filetype is .nc...\n",
    "        if file.endswith('.nc'):\n",
    "            # Load the current dataset\n",
    "            dataset = xr.open_dataset(f'{parent_folder}/{file}')\n",
    "            print(f'Now searching {file}...')\n",
    "            # Check whether the data contains a time coordinate\n",
    "            if 'time' not in dataset:\n",
    "                print('No time coordinate found - skipping current file')\n",
    "                continue\n",
    "            # Remove the time component from the date\n",
    "            date_only = np.datetime64(date, 'D')\n",
    "            \n",
    "            print()\n",
    "            print(date)\n",
    "            print(date_str)\n",
    "            print(date_only)\n",
    "            \n",
    "            print()\n",
    "            # Select time values within the current date\n",
    "            time_within_date = dataset.time.where((dataset.time.dt.floor('D') == date_only), drop=True)\n",
    "            # Check whether dataset uses time as an index\n",
    "            try:\n",
    "                dataset = dataset.sel(time=time_within_date)\n",
    "            except KeyError:\n",
    "                print('No index found for coordinate time - skipping current file')\n",
    "                continue\n",
    "            # Check whether any data remains\n",
    "            if len(dataset.time) == 0:\n",
    "                print(f'{file} in {key} block has no data inside current date - skipping current key...')\n",
    "                break_current = True\n",
    "                break\n",
    "            # Check whether the dataset contains any of the required variables\n",
    "            if any(var in dataset.variables for var in required_variables):\n",
    "                # Drop variables not in required_variables or required_coordinates\n",
    "                dataset = dataset.drop_vars([var for var in dataset.variables if var not in variables_to_keep])\n",
    "                # Loop across the variables in the dataset\n",
    "                for var in dataset:\n",
    "                    # Is the variable required?\n",
    "                    if var in required_variables:\n",
    "                        # Has the variable already been found in the current key?\n",
    "                        if var in variables_found:\n",
    "                            # Drop the variable from the dataset\n",
    "                            dataset = dataset.drop_vars(var)\n",
    "                        else:\n",
    "                            print(f'{var} found in {file}')\n",
    "                            # Add the variable to the list of found variables\n",
    "                            variables_found += [var]\n",
    "                # Check whether the dataset still contains any of the required variables\n",
    "                if any(var in dataset.variables for var in required_variables):\n",
    "                    # Merge the dataset with the key_dataset\n",
    "    \n",
    "                    # TEMPORARY SUBSETTING TO ENSURE TESTABILITY\n",
    "                    if 'lat' in dataset.variables and 'lon' in dataset.variables:\n",
    "                        subset = dataset.sel(lat=0, lon=0, method='nearest')\n",
    "                        key_dataset = key_dataset.merge(subset)\n",
    "                        print(f'File {file} merged')\n",
    "                    elif 'grid_latitude' in dataset.variables and 'grid_longitude' in dataset.variables:\n",
    "                        subset = dataset.sel(grid_latitude=0, grid_longitude=0, method='nearest')\n",
    "                        key_dataset = key_dataset.merge(subset)\n",
    "                        print(f'File {file} merged')\n",
    "                    else:\n",
    "                        print(f'{file} does not use the required coordinates')\n",
    "                        continue\n",
    "\n",
    "                else:\n",
    "                    # No variables found?\n",
    "                    print(f'{file} contains no required variables')\n",
    "            else:\n",
    "                # No variables found?\n",
    "                print(f'{file} contains no required variables')\n",
    "        # If the filetype is not .nc...\n",
    "        else:\n",
    "            # Load the current dataset using iris\n",
    "            iris_cubes = iris.load(f'{parent_folder}/{file}')\n",
    "            # Initialize an empty dataset\n",
    "            dataset = xr.Dataset()\n",
    "            print(f'Now searching {file}...')\n",
    "            # Iterate over the variables in iris_cubes to create the dataset\n",
    "            for i, f in enumerate(iris_cubes):\n",
    "                # Create the temporary dataset\n",
    "                temp_dataset = xr.DataArray.from_iris(iris_cubes[i])                \n",
    "                # Check whether the temp_dataset current variable is required\n",
    "                if temp_dataset.name in required_variables:\n",
    "                    # Check whether the variable has already been found\n",
    "                    if temp_dataset.name not in variables_found:\n",
    "                        try:\n",
    "                            dataset = dataset.merge(temp_dataset)\n",
    "                        except xr.MergeError as e:\n",
    "                            if 'conflicting values for variable \\'level_height\\'' in str(e):\n",
    "                                print(f'Warning: Conflicing values detected for variable \"level_height\" in {temp_dataset.name}; compat=\"override\" used')\n",
    "                                dataset = dataset.merge(temp_dataset, compat='override')\n",
    "                            elif 'conflicting values for variable \\'forecast_period\\'' in str(e):\n",
    "                                print(f'Warning: Conflicing values detected for variable \"forecast_period\" in {temp_dataset.name}; compat=\"override\" used')\n",
    "                                dataset = dataset.merge(temp_dataset, compat='override')\n",
    "                            elif 'conflicting values for variable \\'forecast_reference_time\\'' in str(e):\n",
    "                                print(f'Warning: Conflicing values detected for variable \"forecast_reference_time\" in {temp_dataset.name}; compat=\"override\" used')\n",
    "                                dataset = dataset.merge(temp_dataset, compat='override')\n",
    "                            elif 'conflicting values for variable \\'height\\'' in str(e):\n",
    "                                print(f'Warning: Conflicing values detected for variable \"height\" in {temp_dataset.name}; compat=\"override\" used')\n",
    "                                dataset = dataset.merge(temp_dataset, compat='override')\n",
    "                            elif 'conflicting values for variable \\'level_height\\'' in str(e):\n",
    "                                print(f'Warning: Conflicing values detected for variable \"level_height\" in {temp_dataset.name}; compat=\"override\" used')\n",
    "                                dataset = dataset.merge(temp_dataset, compat='override')\n",
    "                            else:\n",
    "                                raise e\n",
    "                        except AttributeError as e:\n",
    "                            print(f\"Skipping variable {temp_dataset.name}: {e}\")\n",
    "                            continue  # Skip this iteration and continue with the next one\n",
    "                        except ValueError as e:\n",
    "                            if \"Unpacking PP fields with LBPACK of 1 requires mo_pack to be installed\" in str(e):\n",
    "                                print(\"Warning: mo_pack is not installed; skipping the current loop iteration\")\n",
    "                                continue  # Skip the current iteration and move to the next one\n",
    "                            else:\n",
    "                                raise # Re-raise the error if it's not the expected one\n",
    "                    print(f'{temp_dataset.name} found in {file}')\n",
    "                    # Add the variable to the list of found variables\n",
    "                    variables_found += [temp_dataset.name]\n",
    "                    # Identify the variable found\n",
    "            # Check whether the data contains a time coordinate\n",
    "            if 'time' not in dataset.variables:\n",
    "                print('No time coordinate found - skipping current file')\n",
    "                continue\n",
    "            # Remove the time component from the date\n",
    "            date_only = np.datetime64(date, 'D')\n",
    "            # Select time values within the current day\n",
    "            time_within_date = dataset.time.where((dataset.time.dt.floor('D') == date_only), drop=True)\n",
    "            # Check whether dataset uses time as an index\n",
    "            try:\n",
    "                dataset = dataset.sel(time=time_within_date)\n",
    "            except KeyError:\n",
    "                print('No index found for coordinate time - skipping current file')\n",
    "                continue\n",
    "            # Check whether any data remains\n",
    "            if len(dataset.time) == 0:\n",
    "                print(f'{file} in {key} block has no data inside current date - skipping current key...')\n",
    "                break_current = True\n",
    "                break\n",
    "            # Drop variables not in required_variables or required_coordinates\n",
    "            dataset = dataset.drop_vars([var for var in dataset.variables if var not in variables_to_keep])\n",
    "            # Check whether the dataset still contains any of the required variables\n",
    "            if any(var in dataset.variables for var in required_variables):\n",
    "                # Merge the dataset with the key_dataset\n",
    "    \n",
    "                # TEMPORARY SUBSETTING TO ENSURE TESTABILITY\n",
    "                if 'lat' in dataset.variables and 'lon' in dataset.variables:\n",
    "                    subset = dataset.sel(lat=0, lon=0, method='nearest')\n",
    "                    key_dataset = key_dataset.merge(subset)\n",
    "                    print(f'File {file} merged')\n",
    "                elif 'grid_latitude' in dataset.variables and 'grid_longitude' in dataset.variables:\n",
    "                    subset = dataset.sel(grid_latitude=0, grid_longitude=0, method='nearest')\n",
    "                    key_dataset = key_dataset.merge(subset)\n",
    "                    print(f'File {file} merged')\n",
    "                else:\n",
    "                    print(f'{file} does not use the required coordinates')\n",
    "                    continue\n",
    "            else:\n",
    "                # No variables found?\n",
    "                print(f'{file} contains no required variables')\n",
    "    \n",
    "    # If there are no data in the current date, skip the current key\n",
    "    if not break_current:\n",
    "        # Set the save path and save the data\n",
    "        print(f'Saving the {key} data...')\n",
    "        save_path = os.path.join(output_folder, f'um_emc2_test_{date_str}_{key}.nc')\n",
    "        key_dataset.to_netcdf(save_path)\n",
    "        print(f'File saved: {save_path}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b86a5-0c6e-4c8b-94cd-492f49cf1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern of partial files to search for\n",
    "pattern = os.path.join(output_folder, f'*{date_str}*')\n",
    "# List all partial files for the current date\n",
    "saved_partial_files = glob.glob(pattern)\n",
    "# Initialize the final dataset\n",
    "final_dataset = xr.Dataset()\n",
    "# Load each partial file and merge into the final dataset\n",
    "for file in saved_partial_files:\n",
    "    temp_dataset = xr.open_dataset(file)\n",
    "    final_dataset = final_dataset.merge(temp_dataset)\n",
    "\n",
    "# Create a variable filled with zeros and add to the dataset\n",
    "zeros_data = xr.DataArray(\n",
    "    data = np.zeros((len(final_dataset['time']), len(final_dataset['level_height']))),\n",
    "    dims = ('time', 'model_level_number'),\n",
    "    coords = {'time': final_dataset['time'], 'level_height': final_dataset['level_height']}\n",
    ")\n",
    "final_dataset['zeros_var'] = zeros_data\n",
    "\n",
    "# Add a 2-dimensional z_values variable to the dataset\n",
    "level_height = final_dataset['level_height'].values\n",
    "height_data = level_height[:, np.newaxis] * np.ones(len(final_dataset['time']))\n",
    "final_dataset['height_var'] = (('level_height', 'time'), height_data)\n",
    "final_dataset['height_var'].attrs['units'] = 'meter'\n",
    "final_dataset['height_var'] = final_dataset['height_var'].transpose('time', 'level_height')\n",
    "\n",
    "# Add units to all fields\n",
    "# Mass mixing ratio\n",
    "final_dataset['mass_fraction_of_cloud_liquid_water_in_air'] = final_dataset['mass_fraction_of_cloud_liquid_water_in_air'].assign_attrs(units=ureg.kg/ureg.kg)\n",
    "final_dataset['mass_fraction_of_cloud_liquid_water_in_air'].attrs['units'] = str(final_dataset['mass_fraction_of_cloud_liquid_water_in_air'].attrs['units'])\n",
    "final_dataset['mass_fraction_of_cloud_ice_crystals_in_air'] = final_dataset['mass_fraction_of_cloud_ice_crystals_in_air'].assign_attrs(units=ureg.kg/ureg.kg)\n",
    "final_dataset['mass_fraction_of_cloud_ice_crystals_in_air'].attrs['units'] = str(final_dataset['mass_fraction_of_cloud_ice_crystals_in_air'].attrs['units'])\n",
    "final_dataset['mass_fraction_of_rain_in_air'] = final_dataset['mass_fraction_of_rain_in_air'].assign_attrs(units=ureg.kg/ureg.kg)\n",
    "final_dataset['mass_fraction_of_rain_in_air'].attrs['units'] = str(final_dataset['mass_fraction_of_rain_in_air'].attrs['units'])\n",
    "final_dataset['mass_fraction_of_cloud_ice_in_air'] = final_dataset['mass_fraction_of_cloud_ice_in_air'].assign_attrs(units=ureg.kg/ureg.kg)\n",
    "final_dataset['mass_fraction_of_cloud_ice_in_air'].attrs['units'] = str(final_dataset['mass_fraction_of_cloud_ice_in_air'].attrs['units'])\n",
    "final_dataset['mass_fraction_of_graupel_in_air'] = final_dataset['mass_fraction_of_graupel_in_air'].assign_attrs(units=ureg.kg/ureg.kg)\n",
    "final_dataset['mass_fraction_of_graupel_in_air'].attrs['units'] = str(final_dataset['mass_fraction_of_graupel_in_air'].attrs['units'])\n",
    "# Number concentration\n",
    "# Add number concentration per kilogram of air to final_dataset\n",
    "final_dataset['number_of_cloud_droplets_per_kg_of_air'] = final_dataset['number_of_cloud_droplets_per_kg_of_air']\n",
    "final_dataset['number_of_ice_particles_per_kg_of_air'] = final_dataset['number_of_ice_particles_per_kg_of_air']\n",
    "final_dataset['number_of_rain_drops_per_kg_of_air'] = final_dataset['number_of_rain_drops_per_kg_of_air']\n",
    "final_dataset['number_of_snow_aggregates_per_kg_of_air'] = final_dataset['number_of_snow_aggregates_per_kg_of_air']\n",
    "final_dataset['number_of_graupel_particles_per_kg_of_air'] = final_dataset['number_of_graupel_particles_per_kg_of_air']\n",
    "# Add units\n",
    "final_dataset['number_of_cloud_droplets_per_kg_of_air'] = final_dataset['number_of_cloud_droplets_per_kg_of_air'].assign_attrs(units=ureg.kg**(-1))\n",
    "final_dataset['number_of_cloud_droplets_per_kg_of_air'].attrs['units'] = str(final_dataset['number_of_cloud_droplets_per_kg_of_air'].attrs['units'])\n",
    "final_dataset['number_of_ice_particles_per_kg_of_air'] = final_dataset['number_of_ice_particles_per_kg_of_air'].assign_attrs(units=ureg.kg**(-1))\n",
    "final_dataset['number_of_ice_particles_per_kg_of_air'].attrs['units'] = str(final_dataset['number_of_ice_particles_per_kg_of_air'].attrs['units'])\n",
    "final_dataset['number_of_rain_drops_per_kg_of_air'] = final_dataset['number_of_rain_drops_per_kg_of_air'].assign_attrs(units=ureg.kg**(-1))\n",
    "final_dataset['number_of_rain_drops_per_kg_of_air'].attrs['units'] = str(final_dataset['number_of_rain_drops_per_kg_of_air'].attrs['units'])\n",
    "final_dataset['number_of_snow_aggregates_per_kg_of_air'] = final_dataset['number_of_snow_aggregates_per_kg_of_air'].assign_attrs(units=ureg.kg**(-1))\n",
    "final_dataset['number_of_snow_aggregates_per_kg_of_air'].attrs['units'] = str(final_dataset['number_of_snow_aggregates_per_kg_of_air'].attrs['units'])\n",
    "final_dataset['number_of_graupel_particles_per_kg_of_air'] = final_dataset['number_of_graupel_particles_per_kg_of_air'].assign_attrs(units=ureg.kg**(-1))\n",
    "final_dataset['number_of_graupel_particles_per_kg_of_air'].attrs['units'] = str(final_dataset['number_of_graupel_particles_per_kg_of_air'].attrs['units'])\n",
    "# Stratiform fraction\n",
    "final_dataset['cloud_area_fraction_in_atmosphere_layer'] = final_dataset['cloud_area_fraction_in_atmosphere_layer'].assign_attrs(units=ureg.meter/ureg.meter)\n",
    "final_dataset['cloud_area_fraction_in_atmosphere_layer'].attrs['units'] = str(final_dataset['cloud_area_fraction_in_atmosphere_layer'].attrs['units'])\n",
    "# Pressure\n",
    "final_dataset['air_pressure'] = final_dataset['air_pressure'].assign_attrs(units=ureg.pascal)\n",
    "final_dataset['air_pressure'].attrs['units'] = str(final_dataset['air_pressure'].attrs['units'])\n",
    "# Temperature\n",
    "final_dataset['air_temperature'] = final_dataset['air_temperature'].assign_attrs(units=ureg.kelvin)\n",
    "final_dataset['air_temperature'].attrs['units'] = str(final_dataset['air_temperature'].attrs['units'])\n",
    "# Specific humidity\n",
    "final_dataset['specific_humidity'] = final_dataset['specific_humidity']\n",
    "final_dataset['specific_humidity'] = final_dataset['specific_humidity'].assign_attrs(units=ureg.kg/ureg.kg)\n",
    "final_dataset['specific_humidity'].attrs['units'] = str(final_dataset['specific_humidity'].attrs['units'])\n",
    "\n",
    "# Change primary height dimension to use level_height values\n",
    "level_heights = final_dataset['level_height']\n",
    "final_dataset['model_level_number'] = level_heights\n",
    "# Remove/rename coordinates\n",
    "# final_dataset = final_dataset.drop_vars(['level_height', 'sigma', 'forecast_reference_time'])\n",
    "final_dataset = final_dataset.drop_vars(['level_height'])\n",
    "final_dataset = final_dataset.rename({'model_level_number': 'level_height'})\n",
    "\n",
    "# Add general attributes to the dataset\n",
    "# final_dataset.attrs['description'] = 'UM regional model data subsetted to the MARCUS RSV location'\n",
    "# final_dataset.attrs['authors'] = 'Calum L. Knight & Sonya L. Fiddes'\n",
    "final_dataset.attrs['creation_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Set the final filename\n",
    "final_filename = f'um_emc2_test_{date_str}.nc'\n",
    "# Save the final dataset\n",
    "final_dataset.to_netcdf(os.path.join(output_folder, final_filename))\n",
    "\n",
    "# Delete partial files\n",
    "for file in saved_partial_files:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f010a-1d7d-4f97-be9b-f011e719e49b",
   "metadata": {},
   "source": [
    "# Execute um_emc2_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4367b98-3a29-4415-af1c-0258e5e17860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date\n",
    "# date = datetime(2018, 2, 4)\n",
    "date = '20180204'\n",
    "\n",
    "# Parent folder\n",
    "parent_folder = '/g/data/jk72/ck4840/projects/um_emc2/data/um_reg_input_files'\n",
    "# parent_folder = '/g/data/jk72/slf563/cylc-run/u-db930/share/cycle/20180203T1200Z/Mawson/resn_1/RAL3p2/um/processed'\n",
    "# parent_folder = '/g/data/jk72/ck4840/projects/um_emc2/data/um_reg_input_files' - backup .nc files\n",
    "\n",
    "# Output folder\n",
    "output_folder = '/g/data/jk72/ck4840/projects/um_emc2/data/output'\n",
    "\n",
    "# Coordinates\n",
    "# '/g/data/jk72/ck4840/projects/emc2/data/marcus_coordinates/marcus_coordinates_20180201_20180207_1min.nc'\n",
    "latitudes, longitudes = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "91f9e3f9-1a81-4e51-8a55-eaa7c7eeae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 .nc files found \n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "\n",
      "um_emc2_input_validator executed successfully\n",
      "000\n",
      "umnsaa_pa000.nc\n",
      "umnsaa_palcf000.nc\n",
      "umnsaa_pcloud000.nc\n",
      "umnsaa_pmixing000.nc\n",
      "umnsaa_pradar000.nc\n",
      "umnsaa_pvera000.nc\n",
      "umnsaa_pverb000.nc\n",
      "umnsaa_pverc000.nc\n",
      "umnsaa_pverd000.nc\n",
      "umnsaa_pwind000.nc\n",
      "012\n",
      "umnsaa_pa012.nc\n",
      "umnsaa_palcf012.nc\n",
      "umnsaa_pcloud012.nc\n",
      "umnsaa_pmixing012.nc\n",
      "umnsaa_pradar012.nc\n",
      "umnsaa_pvera012.nc\n",
      "umnsaa_pverb012.nc\n",
      "umnsaa_pverc012.nc\n",
      "umnsaa_pverd012.nc\n",
      "umnsaa_pwind012.nc\n",
      "024\n",
      "umnsaa_pa024.nc\n",
      "umnsaa_palcf024.nc\n",
      "umnsaa_pcloud024.nc\n",
      "umnsaa_pmixing024.nc\n",
      "umnsaa_pradar024.nc\n",
      "umnsaa_pvera024.nc\n",
      "umnsaa_pverb024.nc\n",
      "umnsaa_pverc024.nc\n",
      "umnsaa_pverd024.nc\n",
      "umnsaa_pwind024.nc\n",
      "\n",
      "um_emc2_dictionary_sorter executed successfully\n",
      "Now searching 000 files...\n",
      "Now searching umnsaa_pa000.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "No index found for coordinate time - skipping current file\n",
      "Now searching umnsaa_palcf000.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "umnsaa_palcf000.nc in 000 block has no data inside current date - skipping current key...\n",
      "\n",
      "Now searching 012 files...\n",
      "Now searching umnsaa_pa012.nc...\n",
      "No time coordinate found - skipping current file\n",
      "Now searching umnsaa_palcf012.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "mass_fraction_of_cloud_ice_crystals_in_air found in umnsaa_palcf012.nc\n",
      "air_pressure found in umnsaa_palcf012.nc\n",
      "air_temperature found in umnsaa_palcf012.nc\n",
      "cloud_area_fraction_in_atmosphere_layer found in umnsaa_palcf012.nc\n",
      "mass_fraction_of_cloud_ice_in_air found in umnsaa_palcf012.nc\n",
      "mass_fraction_of_cloud_liquid_water_in_air found in umnsaa_palcf012.nc\n",
      "mass_fraction_of_graupel_in_air found in umnsaa_palcf012.nc\n",
      "mass_fraction_of_rain_in_air found in umnsaa_palcf012.nc\n",
      "File umnsaa_palcf012.nc merged\n",
      "Now searching umnsaa_pcloud012.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "specific_humidity found in umnsaa_pcloud012.nc\n",
      "File umnsaa_pcloud012.nc merged\n",
      "Now searching umnsaa_pmixing012.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "number_of_cloud_droplets_per_kg_of_air found in umnsaa_pmixing012.nc\n",
      "number_of_rain_drops_per_kg_of_air found in umnsaa_pmixing012.nc\n",
      "number_of_ice_particles_per_kg_of_air found in umnsaa_pmixing012.nc\n",
      "number_of_snow_aggregates_per_kg_of_air found in umnsaa_pmixing012.nc\n",
      "number_of_graupel_particles_per_kg_of_air found in umnsaa_pmixing012.nc\n",
      "File umnsaa_pmixing012.nc merged\n",
      "Now searching umnsaa_pradar012.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "umnsaa_pradar012.nc contains no required variables\n",
      "Now searching umnsaa_pvera012.nc...\n",
      "No time coordinate found - skipping current file\n",
      "Now searching umnsaa_pverb012.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "umnsaa_pverb012.nc contains no required variables\n",
      "Now searching umnsaa_pverc012.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "umnsaa_pverc012.nc contains no required variables\n",
      "Now searching umnsaa_pverd012.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "umnsaa_pverd012.nc contains no required variables\n",
      "Now searching umnsaa_pwind012.nc...\n",
      "No time coordinate found - skipping current file\n",
      "Saving the 012 data...\n",
      "File saved: /g/data/jk72/ck4840/projects/um_emc2/data/output/um_emc2_test_20180204_012.nc\n",
      "\n",
      "Now searching 024 files...\n",
      "Now searching umnsaa_pa024.nc...\n",
      "No time coordinate found - skipping current file\n",
      "Now searching umnsaa_palcf024.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "mass_fraction_of_cloud_ice_crystals_in_air found in umnsaa_palcf024.nc\n",
      "air_pressure found in umnsaa_palcf024.nc\n",
      "air_temperature found in umnsaa_palcf024.nc\n",
      "cloud_area_fraction_in_atmosphere_layer found in umnsaa_palcf024.nc\n",
      "mass_fraction_of_cloud_ice_in_air found in umnsaa_palcf024.nc\n",
      "mass_fraction_of_cloud_liquid_water_in_air found in umnsaa_palcf024.nc\n",
      "mass_fraction_of_graupel_in_air found in umnsaa_palcf024.nc\n",
      "mass_fraction_of_rain_in_air found in umnsaa_palcf024.nc\n",
      "File umnsaa_palcf024.nc merged\n",
      "Now searching umnsaa_pcloud024.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "specific_humidity found in umnsaa_pcloud024.nc\n",
      "File umnsaa_pcloud024.nc merged\n",
      "Now searching umnsaa_pmixing024.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "number_of_cloud_droplets_per_kg_of_air found in umnsaa_pmixing024.nc\n",
      "number_of_rain_drops_per_kg_of_air found in umnsaa_pmixing024.nc\n",
      "number_of_ice_particles_per_kg_of_air found in umnsaa_pmixing024.nc\n",
      "number_of_snow_aggregates_per_kg_of_air found in umnsaa_pmixing024.nc\n",
      "number_of_graupel_particles_per_kg_of_air found in umnsaa_pmixing024.nc\n",
      "File umnsaa_pmixing024.nc merged\n",
      "Now searching umnsaa_pradar024.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "umnsaa_pradar024.nc contains no required variables\n",
      "Now searching umnsaa_pvera024.nc...\n",
      "No time coordinate found - skipping current file\n",
      "Now searching umnsaa_pverb024.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "umnsaa_pverb024.nc contains no required variables\n",
      "Now searching umnsaa_pverc024.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "umnsaa_pverc024.nc contains no required variables\n",
      "Now searching umnsaa_pverd024.nc...\n",
      "\n",
      "2018-02-04 00:00:00\n",
      "20180204\n",
      "2018-02-04\n",
      "\n",
      "umnsaa_pverd024.nc contains no required variables\n",
      "Now searching umnsaa_pwind024.nc...\n",
      "No time coordinate found - skipping current file\n",
      "Saving the 024 data...\n",
      "File saved: /g/data/jk72/ck4840/projects/um_emc2/data/output/um_emc2_test_20180204_024.nc\n",
      "\n",
      "um_emc2_data_processor executed successfully\n",
      "um_emc2_final_saver executed successfully\n",
      "um_emc2_main executed successfully\n"
     ]
    }
   ],
   "source": [
    "um_emc2_main(parent_folder = parent_folder, date = date, output_folder = output_folder, latitudes = 0, longitudes = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "af90f688-0561-4323-b7e4-26695c7ca9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:        (index: 1008)\n",
       "Coordinates:\n",
       "  * index          (index) int64 2304 2305 2306 2307 ... 3308 3309 3310 3311\n",
       "    datetime_1min  (index) object ...\n",
       "Data variables:\n",
       "    longitude      (index) float64 ...\n",
       "    latitude       (index) float64 ...\n",
       "    voyage_id      (index) int64 ...\n",
       "    datetime_day   (index) datetime64[ns] ...</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-2ac3a696-98a9-4c61-baf4-372acf81eec3' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-2ac3a696-98a9-4c61-baf4-372acf81eec3' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>index</span>: 1008</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-5b62886a-9715-44de-9f78-0cdfd62aa0fc' class='xr-section-summary-in' type='checkbox'  checked><label for='section-5b62886a-9715-44de-9f78-0cdfd62aa0fc' class='xr-section-summary' >Coordinates: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>index</span></div><div class='xr-var-dims'>(index)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>2304 2305 2306 ... 3309 3310 3311</div><input id='attrs-41089b74-f93c-44d4-b7fe-dc80b9c970ca' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-41089b74-f93c-44d4-b7fe-dc80b9c970ca' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b1a67d18-ed6f-4f3c-b051-a709b38f6297' class='xr-var-data-in' type='checkbox'><label for='data-b1a67d18-ed6f-4f3c-b051-a709b38f6297' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([2304, 2305, 2306, ..., 3309, 3310, 3311])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>datetime_1min</span></div><div class='xr-var-dims'>(index)</div><div class='xr-var-dtype'>object</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-857b3afe-9f56-48b7-970f-371d52e7ac29' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-857b3afe-9f56-48b7-970f-371d52e7ac29' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-a31a5aea-d973-4348-942e-d19eee80bbb3' class='xr-var-data-in' type='checkbox'><label for='data-a31a5aea-d973-4348-942e-d19eee80bbb3' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1008 values with dtype=object]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-007362ea-f04b-4a6e-b5cc-824eaa849362' class='xr-section-summary-in' type='checkbox'  checked><label for='section-007362ea-f04b-4a6e-b5cc-824eaa849362' class='xr-section-summary' >Data variables: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>longitude</span></div><div class='xr-var-dims'>(index)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-8c869405-0eb6-4eaf-86ed-32b93d1888a4' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-8c869405-0eb6-4eaf-86ed-32b93d1888a4' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-4afcff92-6f6e-4db8-bf54-71076bb81e0e' class='xr-var-data-in' type='checkbox'><label for='data-4afcff92-6f6e-4db8-bf54-71076bb81e0e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1008 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>latitude</span></div><div class='xr-var-dims'>(index)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-5cd841db-94d6-4de3-b30f-5e82e9c361ae' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-5cd841db-94d6-4de3-b30f-5e82e9c361ae' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-545ebc77-dcd4-4872-81af-1b5d90d9485d' class='xr-var-data-in' type='checkbox'><label for='data-545ebc77-dcd4-4872-81af-1b5d90d9485d' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1008 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>voyage_id</span></div><div class='xr-var-dims'>(index)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-6b5d4972-5bc2-4010-a7da-69227605f775' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-6b5d4972-5bc2-4010-a7da-69227605f775' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-c1fe55f4-b348-47d4-940a-847f0d06f2bc' class='xr-var-data-in' type='checkbox'><label for='data-c1fe55f4-b348-47d4-940a-847f0d06f2bc' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1008 values with dtype=int64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>datetime_day</span></div><div class='xr-var-dims'>(index)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-c739c8a0-f994-4e86-bfb0-cac1adff9c9e' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-c739c8a0-f994-4e86-bfb0-cac1adff9c9e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e7ff40bd-d43a-4e3a-92d3-406f3fd4139c' class='xr-var-data-in' type='checkbox'><label for='data-e7ff40bd-d43a-4e3a-92d3-406f3fd4139c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1008 values with dtype=datetime64[ns]]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-0cbea256-9c00-467c-829d-ae8646312364' class='xr-section-summary-in' type='checkbox'  ><label for='section-0cbea256-9c00-467c-829d-ae8646312364' class='xr-section-summary' >Indexes: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>index</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-1d8f35df-de96-4e25-b391-ffa85456dfde' class='xr-index-data-in' type='checkbox'/><label for='index-1d8f35df-de96-4e25-b391-ffa85456dfde' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313,\n",
       "       ...\n",
       "       3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311],\n",
       "      dtype=&#x27;int64&#x27;, name=&#x27;index&#x27;, length=1008))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-19aab058-d0fc-40b1-9903-b1ddf0c2564a' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-19aab058-d0fc-40b1-9903-b1ddf0c2564a' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (index: 1008)\n",
       "Coordinates:\n",
       "  * index          (index) int64 2304 2305 2306 2307 ... 3308 3309 3310 3311\n",
       "    datetime_1min  (index) object ...\n",
       "Data variables:\n",
       "    longitude      (index) float64 ...\n",
       "    latitude       (index) float64 ...\n",
       "    voyage_id      (index) int64 ...\n",
       "    datetime_day   (index) datetime64[ns] ..."
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '/g/data/jk72/ck4840/projects/emc2/data/marcus_coordinates/marcus_coordinates_20180201_20180207_1min.nc'\n",
    "marcus_coordinates = xr.open_dataset(file)\n",
    "marcus_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "686eea6a-e743-42ee-9346-6f219ca2a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create test plots of atmospheric properties from the processed UM output\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# # Create a meshgrid for time and height to be used for all plots produced from the current file\n",
    "# time_mesh, height_mesh = np.meshgrid(test['time'], test['level_height'])\n",
    "\n",
    "# # Set the height limit for the plots\n",
    "# ylim_max = 15000\n",
    "\n",
    "# # QCL\n",
    "# # Extract the desired variable\n",
    "# QCL = test['mass_fraction_of_cloud_liquid_water_in_air']\n",
    "# # Set QCL values equal to 0 to grey\n",
    "# QCL_grey = np.where(QCL == 0, np.nan, QCL)\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, QCL_grey.T, cmap = 'viridis', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'QCL (kg/kg)')\n",
    "# plt.title('Cloud liquid mass mixing ratio vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_QCL = plt.gcf()\n",
    "\n",
    "# # QCF\n",
    "# # Extract the desired variable\n",
    "# QCF = test['mass_fraction_of_cloud_ice_crystals_in_air']\n",
    "# # Set QCF values equal to 0 to grey\n",
    "# QCF_grey = np.where(QCF == 0, np.nan, QCF)\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, QCF_grey.T, cmap = 'viridis', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'QCF (kg/kg)')\n",
    "# plt.title('Cloud ice mass mixing ratio vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_QCF = plt.gcf()\n",
    "\n",
    "# # QPL\n",
    "# # Extract the desired variable\n",
    "# QPL = test['mass_fraction_of_rain_in_air']\n",
    "# # Set QPL values equal to 0 to grey\n",
    "# QPL_grey = np.where(QPL == 0, np.nan, QPL)\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, QPL_grey.T, cmap = 'viridis', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'QPL (kg/kg)')\n",
    "# plt.title('Precip. liquid mass mixing ratio vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_QPL = plt.gcf()\n",
    "\n",
    "# # QPF\n",
    "# # Extract the desired variable\n",
    "# QPF = test['mass_fraction_of_cloud_ice_in_air']\n",
    "# # Set QPF values equal to 0 to grey\n",
    "# QPF_grey = np.where(QPF == 0, np.nan, QPF)\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, QPF_grey.T, cmap = 'viridis', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'QPF (kg/kg)')\n",
    "# plt.title('Precip. ice mass mixing ratio vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_QPF = plt.gcf()\n",
    "\n",
    "# # QPG\n",
    "# # Extract the desired variable\n",
    "# QPG = test['mass_fraction_of_graupel_in_air']\n",
    "# # Set QPG values equal to 0 to grey\n",
    "# QPG_grey = np.where(QPG == 0, np.nan, QPG)\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, QPG_grey.T, cmap = 'viridis', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'QPG (kg/kg)')\n",
    "# plt.title('Precip. graupel mass mixing ratio vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_QPG = plt.gcf()\n",
    "\n",
    "# # NCL\n",
    "# # Extract the desired variable\n",
    "# NCL = test['number_of_cloud_droplets_per_kg_of_air']\n",
    "# # Set NCL values equal to 0 to grey\n",
    "# NCL_grey = np.where(NCL == 0, np.nan, NCL)\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, NCL_grey.T, cmap = 'inferno', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'NCL (m^-3)')\n",
    "# plt.title('Cloud liquid number concentration vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_NCL = plt.gcf()\n",
    "\n",
    "# # NCF\n",
    "# # Extract the desired variable\n",
    "# NCF = test['number_of_ice_particles_per_kg_of_air']\n",
    "# # Set NCF values equal to 0 to grey\n",
    "# NCF_grey = np.where(NCF == 0, np.nan, NCF)\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, NCF_grey.T, cmap = 'inferno', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'NCF (m^-3)')\n",
    "# plt.title('Cloud ice number concentration vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_NCF = plt.gcf()\n",
    "\n",
    "# # NPL\n",
    "# # Extract the desired variable\n",
    "# NPL = test['number_of_rain_drops_per_kg_of_air']\n",
    "# # Set NPL values equal to 0 to grey\n",
    "# NPL_grey = np.where(NPL == 0, np.nan, NPL)\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, NPL_grey.T, cmap = 'inferno', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'NPL (m^-3)')\n",
    "# plt.title('Precip. liquid number concentration vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_NPL = plt.gcf()\n",
    "\n",
    "# # NPF\n",
    "# # Extract the desired variable\n",
    "# NPF = test['number_of_snow_aggregates_per_kg_of_air']\n",
    "# # Set NPF values equal to 0 to grey\n",
    "# NPF_grey = np.where(NPF == 0, np.nan, NPF)\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, NPF_grey.T, cmap = 'inferno', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'NPF (m^-3)')\n",
    "# plt.title('Precip. ice number concentration vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_NPF = plt.gcf()\n",
    "\n",
    "# # NPG\n",
    "# # Extract the desired variable\n",
    "# NPG = test['number_of_graupel_particles_per_kg_of_air']\n",
    "# # Set NPG values equal to 0 to grey\n",
    "# NPG_grey = np.where(NPG == 0, np.nan, NPG)\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, NPG_grey.T, cmap = 'inferno', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'NPG (m^-3)')\n",
    "# plt.title('Precip. graupel number concentration vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_NPG = plt.gcf()\n",
    "\n",
    "# # Cloud stratiform fraction\n",
    "# # Extract the desired variable\n",
    "# strat_frac = test['cloud_area_fraction_in_atmosphere_layer']\n",
    "# # Set strat_frac values equal to 0 to grey\n",
    "# strat_frac_grey = np.where(strat_frac == 0, np.nan, strat_frac)\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, strat_frac_grey.T, cmap = 'magma', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'Cloud stratiform fraction')\n",
    "# plt.title('Cloud area fraction vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_strat_frac = plt.gcf()\n",
    "\n",
    "# # Specific humidity\n",
    "# # Extract the desired variable\n",
    "# hus = test['specific_humidity']\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, hus.T, cmap = 'cividis', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'Specific humidity')\n",
    "# plt.title('Specific humidity vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_hus = plt.gcf()\n",
    "\n",
    "# # Pressure\n",
    "# # Extract the desired variable\n",
    "# pressure = test['air_pressure']\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, pressure.T, cmap = 'cividis', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'Pressure (Pa)')\n",
    "# plt.title('Pressure vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_pressure = plt.gcf()\n",
    "\n",
    "# # Temperature\n",
    "# # Extract the desired variable\n",
    "# temperature = test['air_temperature']\n",
    "# # Create the plot with a grey background\n",
    "# plt.figure(figsize = (12, 6))\n",
    "# plt.gca().set_facecolor('lightgrey')\n",
    "# # Create and organise a pcolormesh plot\n",
    "# pcm = plt.pcolormesh(time_mesh, height_mesh, temperature.T, cmap = 'cividis', shading = 'auto')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Height (m)')\n",
    "# plt.colorbar(pcm, label = 'Temperature (K)')\n",
    "# plt.title('Temperature vs. time and height')\n",
    "# plt.ylim(0, ylim_max)\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.tight_layout()\n",
    "# daily_UM_reg_temperature = plt.gcf()\n",
    "\n",
    "# print('Finished plotting')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMC2 environment",
   "language": "python",
   "name": "emc2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
